{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Math\n",
    "import math\n",
    "\n",
    "# Huggingface libraries\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# Pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "# typing\n",
    "from typing import Any\n",
    "\n",
    "# Tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Importing library of warnings\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-0.4378, -0.0305,  0.5566],\n",
       "           [ 1.6604,  0.7059, -1.0342]]]], grad_fn=<EmbeddingBackward0>),\n",
       " Parameter containing:\n",
       " tensor([[ 0.9677, -2.3553,  1.9556],\n",
       "         [-0.4378, -0.0305,  0.5566],\n",
       "         [ 1.6604,  0.7059, -1.0342],\n",
       "         [-0.2311, -1.0571, -1.2353],\n",
       "         [ 0.9896, -0.3503, -0.2724],\n",
       "         [ 1.2129, -0.7979,  0.4722],\n",
       "         [ 2.0810,  0.9564,  0.1199],\n",
       "         [-0.6105, -0.2161,  0.8669],\n",
       "         [ 0.6462, -0.7145,  0.0053],\n",
       "         [-1.9800, -0.0670, -0.9011]], requires_grad=True))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nn.Embedding有两个参数：num_embeddings和embedding_dim。可以理解为num_embeddings是词典的大小，embedding_dim是每个词嵌入的维度。把Embedding层当做是一个字典，输入一个索引，返回一个词向量。\n",
    "# 比如“I love you”这句话，词典大小是3，可以用[0, 1, 2]表示，然后通过Embedding层，针对每一个索引，去找它的词向量。比如说“I”，索引是0，那么从Embedding层取第0行的向量用来表示“I”，同理总共可以得到三个词向量，然后把这三个词向量拼接起来，就是整个句子的词嵌入表示。\n",
    "# Embedding层的参数是一个二维矩阵，该矩阵就是一个包含词向量的字典。行数是num_embeddings，也就是词典大小，列数是embedding_dim。\n",
    "# Embedding层的作用是把输入x里的每个元素用词向量表示，因此x的维度是不限的。\n",
    "embedding = nn.Embedding(10, 3)\n",
    "embedding(torch.tensor([[[1,2]]])),embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Input Embeddings\n",
    "# Transform the input into a sequence of embeddings, \n",
    "# Input example: vocabulary:[I,love, you]  input: I love you -> [I, love, you] -> [0, 1, 2] -> [[1, 0, 0], [0, 1, 0], [0, 0, 1]] dimiension:(n, vocab_size)\n",
    "# The Input Embedding will reduce the dimension of the input to the desired dimension: vocab_size(num_embedding) -> d_model(embedding_dim) \n",
    "class InputEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
